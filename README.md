# Benchmarking Robustness of Deep Learning Classifiers Using Two-Factor Perturbation
The accuracy of Deep Learning classifiers is unstable in that it often changes significantly when retested on imperfect images,  perturbed images, or adversarial images. This paper adds to the small but fundamental body of work on benchmarking the robustness of deep learning classifiers on defective images. Unlike existed single-factor digital perturbation work, we provide state-of-the-art two-factor perturbation that provides two natural perturbations on images applied in different sequences. The two-factor perturbation includes (1) two digital perturbations (salt & pepper noise and Gaussian noise) applied in both sequences. (2) one digital perturbation (salt & pepper noise) and a geometric perturbation (rotation) applied in different sequences. To measure robust DL classifiers, previous scientists provided 15 types of single-factor perturbation. We created 68 two-factor perturbed image test sets. To be best of our knowledge, this is the first report that two-factor perturbation improves both robustness and accuracy of deep learning classifiers. Previous researchers have usually offered tables, line diagrams, and bar charts to display accuracy of DL classifiers, but these approaches cannot quantitively evaluate robustness of deep learning classifiers. We innovate a new two-dimensional, statistical visualization, including mean accuracy and coefficient of variation (CV), to benchmark the robustness of deep learning classifiers. All source codes and related image sets are shared on a website (http://www.animpala.com or https://github.com/daiweiworking/Benchmarking-Robustness-of-Deep-Learning-Classifiers-Using-Two-Factor-Perturbation) to support future academic research and industry projects.
